---
title: "Cancer Data Modeling"
output: 
  html_document:
   toc: FALSE
---

We have already done a basic exploration and analysis of the data, now we will 
work on further modeling to better understand our data and infer any causation 
present- our main predictors we will analyze will be ethnicity and race
```{r}
  #Before we begin let's load some packages that we will need for the data 
#exploration, fitting, and modeling of the data 
#please note that If you do not have the packages installed you will have to
#first install them with the "install.packages()" command
library(tidyverse) #for streamlining manipulating data
library(tidymodels) # for streamlining fitting data to models
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(ggplot2) #for plotting
library(gapminder) #For reordering bar charts to be more easily understood
library(rpart) #for fitting tree model
library(glmnet) #for fitting LASSO model
library(ranger) #for fitting random forest model
library(vip) #for identifying most important variables in our models (the "VIPS")
library(skimr) #for viewing alternative information about variables
library(doParallel) # for parallel processing for quicker tuning
```

Next we will load our processed data:
```{r}
Data_Location<-here::here("data","processed_data","processeddata.rds") 

#Load Data
Cancer_Data<-readRDS(Data_Location)
```

Summary of our data tells us that we are dealing with almost all character 
type variables, with the only numerical being deaths
```{r}
summary(Cancer_Data)
```

Using the glimpse command tells us that we have variable names including Age,Sex,Deaths,Ethnicity,Race,and CancerSite in our final dataset
```{r}
glimpse(Cancer_Data)
```

In addition, utilizing the "skimr" package allows us to more closely 
view the individual variables
```{r}
skim(Cancer_Data)
```

*Please note we have removed 57 missing values (NA) from our data to allow 
for better fitting*

we will use a null model to establish a baseline and compare it with a single tree model, LASSO model, and a random forest model to determine best fit

# Tuning and Modeling


# Tuning
Now that we have finished examining our data, we will start our analysis by
separating our data into a training set for tuning and evaluating the models,
and a test set to compare our results to ensure strength of fit
We first set our seed, which is simply initializing a pseudorandom number generator (makes sure we get the same results each time we run the data (can be any number of choice though)),
```{r}
set.seed(123)
```

Now we will split the data
We will now split the data by 70% for our training data and 30% for our testing
We will be using (Cancer) Deaths as our outcome which we will be evaluating
```{r}
data_split <- initial_split(Cancer_Data, prop = 7/10, strata = Deaths)

#7/10 stands for 70% training, (strata = "Deaths") # and the rest (30%) for testing)
```

Now we will organize our sets of training and test data
```{r}
train_data <- training(data_split)

test_data <- testing(data_split)
```

We will now utilize a 5-fold cross validation, 5 times repeated, we will 
stratify on "Race" for the CV folds
```{r}
Cross_Validation_5F <- vfold_cv(train_data, v = 5, repeats = 5, strata = "Deaths")
```

Now we will create our recipe for our data and fitting,
We will code the categorical variables as dummy variables
```{r}
recipe_Deaths <-recipe(Deaths ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors())
```

# Modeling
We will fit a null model, single tree model, LASSO model, and a random forest model (total of four)

Our steps should be as follows...

# 1. Model Specification

# 2. Workflow Definition

# 3. Tuning Grid Specification

# 4. Tuning Using Cross- Validation and the tune_grid() function


# NULL MODEL
Before we can adequately assess if any of our models posess good fit of our data
we need to first create our null model that we can compare the rest of our models
to, if none of our other models perform better than the Null, they are not worth 
pursuing

We need to specify our model before we start computing
*We will use the function "null_model()"*
*Note that we use "classification" for our mode instead of "regression",*
*this is because the mode is predicted for all outcomes, where as continuous,* *number based data would be best with "regression"*
```{r}
null_model1<- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression") %>%
  translate()
```

We will now create our worflow
```{r}
nullmodel_workflow <- workflow() %>% 
  add_model(null_model1) %>%
  add_recipe(recipe_Deaths)
```


We will now compute the performance of a null model for our training and test data
*(doesn't use any predictor information)*


We will now fit our models
We use our deisgnated workflow in fit_resamples followed by our designated name for cross-validation

*Note that our main outcome of interest is cancer deaths, our main predictors of interest are the variables "Race" and "Ethnicity"*
```{r}
train_null_recipe <- fit_resamples(nullmodel_workflow, Cross_Validation_5F, control=control_resamples(verbose=TRUE))

print(train_null_recipe)

null_recipe_metrics<-collect_metrics(train_null_recipe)
```
This warning would usually be bad on any other model, but since we are receiving it on our null model, we know that it is working as intended

When comparing our models, if the RMSE (our chosen measure of significance) of our other models is worse (in this case a higher value than what is given here, 23678) than what the null model's is, there is indication that the model would not do well at fitting the data. 


*SINGLE TREE MODEL*
```{r}
#Specify Model
tune_spec_TREE <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")
tune_spec_TREE
```

```{r}
#We will now define the workflow for the tree 
workflow_TREE <- workflow() %>%
            add_model(tune_spec_TREE) %>%
            add_recipe(recipe_Deaths) 
```


```{r}
#We will now specify the tuning grid
grid_TREE <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
grid_TREE
```


```{r}
#We will now tune using cross validation and the tune_grid() function
res_TREE<-
  workflow_TREE %>%
  tune_grid(resamples = Cross_Validation_5F , grid = grid_TREE, metrics = metric_set(rmse))
```


```{r}
#Now we will run the autoplot() function to look at some diagnostics
res_TREE %>%
  autoplot()
```


```{r}
#Now we will select the best decision tree model
TOP_TREE <- res_TREE %>% 
  select_best("rmse")

TOP_TREE
```


```{r}
#Now we need to finalize the workflow
single_tree_workflow_FINAL <- workflow_TREE %>% finalize_workflow(TOP_TREE)
single_tree_workflow_FINAL
```


```{r}
#Now we will utilize the fit() function to fit to the training data
fit_FINAL_TREE <- single_tree_workflow_FINAL %>% last_fit(data_split)

#Now we will collect the data from our fit
fit_FINAL_TREE %>% collect_metrics()
```


```{r}
#We will also collect the predictions
pred_TREE <- fit_FINAL_TREE %>% collect_predictions()

#We will now make two plots, one that shows model predictions from the tuned
#model compared to actual outcomes, and one that plots residuals (RMSE)
pred_tree_plot <- ggplot(data = pred_TREE, aes(x = .pred, y = Deaths)) + 
           geom_point() +
           labs(title = "Plot Comparing Model Predictions from Tuned to Actual",
                x = "Predictions", y = "Outcomes")
#view the plot
pred_tree_plot
```


```{r}
#We need to calculate our residuals before we can plot 
#Note that the residuals is the difference between our main predictor and the others
pred_TREE$residuals <- pred_TREE$Deaths - pred_TREE$.pred
```


```{r}
#Now we will plot our residuals
resid_tree_plot <- ggplot(data= pred_TREE, aes(x=.pred , y=residuals)) + geom_point() +
                    labs(title="Plot of Residuals",
                    x="Predictions", y= "Residuals")
#view the plot
resid_tree_plot 

#Now we will compare our residual plot to the null model
tree_model_performance <- res_TREE %>% show_best(n=15)
print(tree_model_performance)
```

*HOW DOES THIS MODEL PERFORM?*
The single tree model with a depth of 15 performed the best with an RMSE of 19904, which is lower than the RMSE of the null model of 23678, this is good since it means that our performance for this model(single tree) was better than the null.


*LASSO*
Now we will construct a LASSO model
```{r}
#We will once again start by constructing our model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) 
```
Please note that mixture refers to a number between zero and one that is the 
proportion of L1 regularization (lasso) in the model. In other, words, because
we are using mixture = 1, we are utilizing a "pure" lasso model here

We will now create our workflow
```{r}
lasso_workflow <-workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(recipe_Deaths)
```

Now we will tune our LASSO model
We will utilize parallel computing again to make it faster
```{r}
library(doParallel)
ncores = 5 
cluster <- makePSOCKcluster(5) 
  registerDoParallel(5) 
```

Now we will create our tuning grid 
```{r}
  lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
  #Now we tune the model
  lasso_tune_res <- lasso_workflow %>%
    tune_grid(resamples = Cross_Validation_5F,
              grid = lasso_reg_grid,
              control = control_grid(save_pred = TRUE),
              metrics = metric_set(rmse))
```

We will now turn off parallel clustering, the reason we turn the clustering off
after each use is to prevent computations and analysis from being slowed in 
later data analysis, fitting, modeling, etc.
```{r}
stopCluster(cluster)
```

```{r}
#Now we will view metrics
lasso_tune_res %>% collect_metrics()
```

Now we will select top models
```{r}
top_lasso <- 
  lasso_tune_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso 
```
Now we will see the best lasso out of our five top models
```{r}
best_lasso <- lasso_tune_res %>% select_best()
best_lasso 
```
Our best LASSO model appears to be the one with a penalty of 1

Now we finalize our workflow with the top model and fit it 
```{r}
lasso_topmodel_workflow <- lasso_workflow %>% finalize_workflow(best_lasso)

lasso_topfit<- lasso_topmodel_workflow %>% fit(train_data)

lasso_topfit
```

Now that we have found our top LASSO model, we will now plot it to show quality of fit
```{r}
autoplot(lasso_tune_res)
  ggsave(here::here("results", "LASSO.png"))
```
```{r}
#we will now calculate the residuals
lasso_residuals <- lasso_topfit %>%
  augment(train_data) %>% 
  select(.pred, Deaths) %>%
  mutate(.resid = Deaths - .pred) 

lasso_residuals
```

```{r}
#Now we will compare model predictions from tuned model to our actual outcomes
lasso_pred_plot <- ggplot(lasso_residuals, aes(x = Deaths, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Deaths (Outcome)", y = "Deaths (Prediction)")
lasso_pred_plot
```

We will now compare our residuals vs predictions
```{r}
lasso_residuals_plot <- ggplot(lasso_residuals, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "LASSO Model Residuals VS Predictions", x = "Deaths Prediction", y = "Residuals")

lasso_residuals_plot 
```

Finally we will compare our LASSO model to our null model
```{r}
lasso_tune_res %>% show_best(n=1) #view RMSE for best lasso model
```
Our LASSO model has a mean value of 21926.34, which is lower than the null model of 23678, but higher than our single tree model value of 19904. We will finally utilize the RandomForest model to compare quality of fit for our model.

*RANDOMFOREST*
So far our single tree model has performed best, we will evaluate the random forest model and compare to determine if we should use the single tree or random forest instead
*Please note that for Random Forest models, "num.threads" and importance*
*is required or else all models will fail*

First we will identify how many cores we can use at one time during parallel processing
```{r}
cores <- parallel::detectCores()
cores
```

We have four cores available for parallel processing, we will use the command "cores" in number of threads to use the max number possible (in our case four)
```{r}
random_forest_model <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```

We will now set our workflow
```{r}
random_forest_workflow <- 
  workflow() %>% 
  add_model(random_forest_model) %>% 
  add_recipe(recipe_Deaths)
```

We will now create our tuning grid for our random forest model
```{r}
random_forest_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
```

Let's examine what we are tuning
```{r}
random_forest_model %>% parameters()
```

Now we will tune with our grid that we created
```{r}
random_forest_resampling <- 
  random_forest_workflow %>%
  tune_grid(resamples = Cross_Validation_5F, 
            grid = random_forest_grid, 
            control = control_grid(save_pred = TRUE), 
            metrics = metric_set(rmse))
```

We will now view our top models
```{r}
random_forest_resampling %>% show_best(metric = "rmse")
```

We will now plot our models
```{r}
autoplot(random_forest_resampling)
  ggsave(here::here("results", "randomforest.png"))
```

```{r}
#we will now select the best performing model
random_forest_best <- random_forest_resampling %>% select_best(metric = "rmse")

random_forest_best
```


```{r}
#now we will finalize workflow with top model
random_forest_final_workflow <- random_forest_workflow %>% finalize_workflow(random_forest_best)
```


```{r}
#now we will fit the model with finalized WF
random_forest_final_fit <- random_forest_final_workflow %>% fit(train_data)

random_forest_final_fit
```

Our random forest model's top model gives an RMSE of 20693, which is slightly worse than our single tree model of 19904, as such, we will use our single tree model to fit our test data

*Final Fitting*
We will finally fit out best performing model (single tree) to our test data to determine quality of fit
```{r}
last_single_tree_fit <- single_tree_workflow_FINAL %>% last_fit(data_split)
last_single_tree_fit %>% collect_metrics()
```
Our final fitting of our model gave an RMSE of 23820, which is much higher than the earlier RMSE of 19904. This signals that there is overfitting present in the data

Let's examine the importance of our variables
```{r}
importance_of_variables <- last_single_tree_fit %>% pluck(".workflow", 1) %>%   
extract_fit_parsnip() %>% vip(num_features = 6)

importance_of_variables

ggsave(importance_of_variables, filename = here("results", "ImportanceOfVariables.png"))
```

The most important variables for our model appear to be being non-Hispanic, and being of White race, with all cancer sites combined being responsible for the third highest importance variable

Finally we will repeat our steps as previously done to plot the final fitted model
```{r}
autoplot(res_TREE)
```

```{r}
#Residuals
final_single_tree_residuals<- last_single_tree_fit %>%
  augment() %>% 
  select(.pred, Deaths) %>%
  mutate(.resid = Deaths - .pred)
```

```{r}
#Tuned VS. Actual model predictions
final_single_tree_plot <- ggplot(final_single_tree_residuals, aes(x = Deaths, y = .pred)) + geom_point() + 
  labs(title = "Single Tree Outcome/Predictions", x = "Outcome", y = "Prediction") + ylim(0, 250) + xlim(0,250)

final_single_tree_plot

ggsave(final_single_tree_plot, filename = here("results", "FinalSingleTreePlot.png"))
```


```{r}
#residuals vs predictions
final_single_tree_residuals_plot <- ggplot(final_single_tree_residuals, aes(x = .pred, y = .resid)) + geom_point() + 
  labs(title = "Single Tree Predictions/Residuals", x = "Predictions", y = "Residuals")
         
final_single_tree_residuals_plot 
```



We will finally compare our null model to our best performing model(single tree in this case)
```{r}
null_recipe_metrics 
last_single_tree_fit %>% collect_metrics() #view RMSE for best lasso model
```

When comparing the two models, the single tree model performs worse than the null (23820 vs. 23678), this infers that the single tree model is not a good fit of our data, likely due to issues with over fitting.
