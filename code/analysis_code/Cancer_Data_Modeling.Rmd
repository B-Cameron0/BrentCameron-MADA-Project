---
title: "Cancer Data Modeling"
output: 
  html_document:
   toc: FALSE
---

We have already done a basic exploration and analysis of the data, now we will 
work on further modeling to better understand our data and infer any causation 
present- our main predictors we will analyze will be ethnicity and race
```{r}
  #Before we begin let's load some packages that we will need for the data 
#exploration, fitting, and modeling of the data 
#please note that If you do not have the packages installed you will have to
#first install them with the "install.packages()" command
library(tidyverse) #for streamlining manipulating data
library(tidymodels) # for streamlining fitting data to models
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(ggplot2) #for plotting
library(gapminder) #For reordering bar charts to be more easily understood
library(rpart) #for fitting tree model
library(glmnet) #for fitting LASSO model
library(ranger) #for fitting random forest model
library(vip) #for identifying most important variables in our models (the "VIPS")
library(skimr) #for viewing alternative information about variables
library(doParallel) # for parallel processing for quicker tuning
```

Next we will load our processed data:
```{r}
Data_Location<-here::here("data","processed_data","processeddata.rds") 

#Load Data
Cancer_Data<-readRDS(Data_Location)
```

Summary of our data tells us that we are dealing with almost all character 
type variables, with the only numerical being deaths
```{r}
summary(Cancer_Data)
```

Using the glimpse command tells us that we have variable names including Age,Sex,Deaths,Ethnicity,Race,and CancerSite in our final dataset
```{r}
glimpse(Cancer_Data)
```

In addition, utilizing the "skimr" package allows us to more closely 
view the individual variables
```{r}
skim(Cancer_Data)
```

*Please note we have removed 57 missing values (NA) from our data to allow 
for better fitting*

we will use a null model to establish a baseline and compare it with a single tree model, LASSO model, and a random forest model to determine best fit

# Tuning and Modeling


# Tuning
Now that we have finished examining our data, we will start our analysis by
separating our data into a training set for tuning and evaluating the models,
and a test set to compare our results to ensure strength of fit
We first set our seed, which is simply initializing a pseudorandom number generator (makes sure we get the same results each time we run the data (can be any number of choice though)),
```{r}
set.seed(123)
```

Then we split the data
```{r}
#We will now split the data by 70% for our training data and 30% for our testing
#We will start by examining race as our strata (Race and Ethnicity are our two main predictors)
data_split <- initial_split(Cancer_Data, prop = 7/10, strata = Race)#7/10 stands for 70%   training, strata = "Race") # and the rest (30%) for testing)
```

Now we will organize our sets of training and test data
```{r}
train_data <- training(data_split)

test_data <- testing(data_split)
```

We will now convert the response variable to a factor
```{r}
null_rec <- recipe(Race ~ ., data=train_data) %>% 
  step_mutate(Race=factor(as.logical(Race)))
```

We will now utilize a 5-fold cross validation, 5 times repeated, we will 
stratify on "Race" for the CV folds
```{r}
Cross_Validation_5F <- vfold_cv(train_data, v = 5, repeats = 5, strata = "Race")
```

Now we will create our recipe for our data and fitting,
We will code the categorical variables as dummy variables
```{r}
recipe_Race <-recipe(Race ~ ., data = train_data) %>%
  step_dummy(all_numerical_predictors())
```

# Modeling
Now that we have decided on what our main predictor variable will be 
(Race (we will do ethnicity later)) and created our data sets (training and 
test), we can now begin modeling

We will fit a null model, single tree model, LASSO model, and a random forest model (total of four)

Our steps should be as follows...

# 1. Model Specification

# 2. Workflow Definition

# 3. Tuning Grid Specification

# 4. Tuning Using Cross- Validation and the tune_grid() function


# NULL MODEL

Before we can adequately assess if any of our models posess good fit of our data
we need to first create our null model that we can compare the rest of our models
to, if none of our other models perform better than the Null, they are not worth 
pursuing

We need to specify our model before we start computing
*We will use the function "null_model()"*
*Note that we use "classification" for our mode instead of "regression",*
*this is because the mode is predicted for all outcomes, where as continuous,* *number based data would be best with "regression"*
```{r}
Null_Model<- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification") %>%
  translate()
```

Now we Combine Workflow and Recipe Together
```{r}
null_wflow <- workflow() %>% 
  add_recipe(recipe_Race) %>%
  add_model(Null_Model)
```


We will now compute the performance of a null model for our training and test data
*(doesn't use any predictor information)*

*Note that our main feature of interest is cancer deaths, followed by how the variables "Race" and "Ethnicity" effect rates of cancer death overall*
# Train Data Computing
```{r}
train_null_recipe <- lm(Deaths ~ ., data = train_data)
```

# Calculating RMSE
```{r}
train_null_recipe %>% augment(newdata = train_data) %>%
                      rmse(truth = Deaths, estimate = .fitted)
```

Test Data Computing
```{r}
test_null_recipe <- lm(Deaths ~ ., data = test_data)
```


# Calculating RMSE
```{r}
test_null_recipe %>% augment(newdata = test_data) %>%
  rmse(truth = Deaths, estimate = .fitted)
```
#When comparing our models, if the RMSE (our chosen measure of significance) of our other models is worse (in this case a higher value than what is given here) than what the null model's is, there is indication that the model would not do well at fitting the data. 

We will start with identifying the best model for race

# SINGLE TREE MODEL

We will now start with our first comparison model (the single tree model)
First we will specify the model
```{r}
tune_spec_TREE <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(5),
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")
```

We will now define the workflow for the tree 
```{r}
workflow_TREE <- workflow() %>%
            add_model(tune_spec_TREE) %>%
            add_recipe(recipe_Race) 
```


We will now specify the tuning grid
```{r}
grid_TREE <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
grid_TREE
```
Let's examine this further
```{r}
grid_TREE%>%
    count(tree_depth)
```


We will now tune using cross validation and the tune_grid() function
We will utilize parallell processing to speed up processing time beforehand

```{r}
ncores = 5 #Ncores is used to select the number of cores you want to recruit
#for processing, different computers will naturally have different ideal numbers

cluster <- makePSOCKcluster(5) #make PSOCKcluster stands for creating a sock
#cluster within the 'snow' package, this allowsa for increased computing time

  registerDoParallel(5) #registers parallel backend with foreach package
```

Now processing should go faster
```{r}
res_TREE<-
  workflow_TREE %>% 
   tune_grid(resamples = Cross_Validation_5F)
      grid = grid_TREE
```


Now we will stop the processing for now before we continue with our analysis
```{r}
stopCluster(cluster)
```

Now we will run the autoplot() function to look at some diagnostics
```{r}
res_TREE %>%
  autoplot()
```

# HOW DOES THIS MODEL PERFORM?
Unfortunately, all models tried have failed, we will hopefully find a 
model that fits the data better


# LASSO
Now we will construct a LASSO model

We will turn all categorical variables into numerical
Cancer_Data1<- sapply(Cancer_Data, as.factors)
```{r}
#We will once again start by constructing our model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) 
```
Please note that mixture refers to a number between zero and one that is the 
proportion of L1 regularization (lasso) in the model. In other, words, because
we are using mixture = 1, we are utilizing a "pure" lasso model here

We will now create our workflow
```{r}
lasso_workflow <-workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(recipe_Race)
```

Now we will tune our LASSO model
We will utilize parallel computing again to make it faster
```{r}
library(doParallel)
ncores = 5 
cluster <- makePSOCKcluster(5) 
  registerDoParallel(5) 
```

Now we will create our tuning grid 
```{r}
  lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
  #Now we tune the model
  lasso_tune_res <- lasso_workflow %>%
    tune_grid(resamples = Cross_Validation_5F,
              grid = lasso_reg_grid,
              control = control_grid(save_pred = TRUE),
              metrics = metric_set(rmse))
```

We will now turn off parallel clustering, the reason we turn the clustering off
after each use is to prevent computations and analysis from being slowed in 
later data analysis, fitting, modeling, etc.
```{r}
stopCluster(cluster)
```

We will now evaluate our LASSO model

The LASSO model also failed in its modeling, we will finally try the RandomForest model

# RANDOMFOREST

  Both of our past models so far have failed, we will now repeat the 
 steps with a random forest model in the hopes of finding significance
  
*Please note that for Random Forest models, "num.threads" and importance*
*is required or else all models will fail*
    
```{r}
randomforest_model <- rand_forest() %>%
  set_args(mtry = tune(),
  trees = tune(), 
  min_n = tune()
  ) %>%
  #Now we set the engine
  set_engine("ranger", 
             num.threads = 5,
             importance = "permutation") %>%
  #We select either the continuous or binary classification
  set_mode("classification")

#We will set our workflow once again
randomforest_workflow <- workflow() %>% 
  add_model(randomforest_model) %>%
  add_recipe(recipe_Race)

#We will now repeat our steps as the first two models to specify our tuning grid
#We will use parallel computing once again to vastly decrease the time it takes 
#to compute the model- since we have already use code previously to create it
#we now only need to use our name designation for our cluster and it will resume
cluster <-makePSOCKcluster(5)
registerDoParallel(5)

#Now we will tune the grid
randomforest_grid <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40, 50, 60),
                                 trees = c(500, 1000))

```
    

We will set our workflow once again
```{r}
  randomforest_workflow <- workflow() %>% 
    add_model(randomforest_model) %>%
    add_recipe(recipe_Race)
```
  
We will now repeat our steps as the first two models to specify our tuning grid
We will use parallel computing once again to vastly decrease the time it takes 
to compute the model- since we have already used code previously to create it
we now only need to use our name designation for our cluster and it will resume
```{r}
  cluster <-makePSOCKcluster(5)
  registerDoParallel(5)
```
  
We will now tune the model while optimizing RMSE
```{r}
 randomforest_tune_res <- randomforest_workflow %>%
    tune_grid(resamples = Cross_Validation_5F, 
              #This is the name of our previous CV object
              grid = randomforest_grid)
#This is the grid of values we want to try
             
```

  Unfortunately, all of the models failed to have a necessary level of significance. There may be issues with conflicitng variables, or Race and Ethnicity simply does not have a significant effect on overall cancer deaths.
