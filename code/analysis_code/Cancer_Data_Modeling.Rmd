---
title: "Cancer Data Modeling"
output: 
  html_document:
   toc: FALSE
---

We have already done a basic exploration and analysis of the data, now we will 
work on further modeling to better understand our data and infer any causation 
present- our main predictors we will analyze will be ethnicity and race
```{r}
  #Before we begin let's load some packages that we will need for the data 
#exploration, fitting, and modeling of the data 
#please note that If you do not have the packages installed you will have to
#first install them with the "install.packages()" command
library(tidyverse) #for streamlining manipulating data
library(tidymodels) # for streamlining fitting data to models
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(ggplot2) #for plotting
library(gapminder) #For reordering bar charts to be more easily understood
library(rpart) #for fitting tree model
library(glmnet) #for fitting LASSO model
library(ranger) #for fitting random forest model
library(vip) #for identifying most important variables in our models (the "VIPS")
library(skimr) #for viewing alternative information about variables
library(doParallel) # for parallel processing for quicker tuning
```

Next we will load our processed data:
```{r}
Data_Location<-here::here("data","processed_data","processeddata.rds") 

#Load Data
Cancer_Data<-readRDS(Data_Location)
```

Summary of our data tells us that we are dealing with almost all character 
type variables, with the only numerical being deaths
```{r}
summary(Cancer_Data)
```

Using the glimpse command tells us that we have variable names including Age,Sex,Deaths,Ethnicity,Race,and CancerSite in our final dataset
```{r}
glimpse(Cancer_Data)
```

In addition, utilizing the "skimr" package allows us to more closely 
view the individual variables
```{r}
skim(Cancer_Data)
```

*Please note we have removed 57 missing values (NA) from our data to allow 
for better fitting*

we will use a null model to establish a baseline and compare it with a single tree model, LASSO model, and a random forest model to determine best fit

# Tuning and Modeling


# Tuning
Now that we have finished examining our data, we will start our analysis by
separating our data into a training set for tuning and evaluating the models,
and a test set to compare our results to ensure strength of fit
We first set our seed, which is simply initializing a pseudorandom number generator (makes sure we get the same results each time we run the data (can be any number of choice though)),
```{r}
set.seed(123)
```

Now we will split the data
We will now split the data by 70% for our training data and 30% for our testing
We will be using (Cancer) Deaths as our outcome which we will be evaluating
```{r}
data_split <- initial_split(Cancer_Data, prop = 7/10, strata = Deaths)

#7/10 stands for 70% training, (strata = "Deaths") # and the rest (30%) for testing)
```

Now we will organize our sets of training and test data
```{r}
train_data <- training(data_split)

test_data <- testing(data_split)
```

We will now utilize a 5-fold cross validation, 5 times repeated, we will 
stratify on "Race" for the CV folds
```{r}
Cross_Validation_5F <- vfold_cv(train_data, v = 5, repeats = 5, strata = "Deaths")
```

Now we will create our recipe for our data and fitting,
We will code the categorical variables as dummy variables
```{r}
recipe_Deaths <-recipe(Deaths ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors())
```

# Modeling
We will fit a null model, single tree model, LASSO model, and a random forest model (total of four)

Our steps should be as follows...

# 1. Model Specification

# 2. Workflow Definition

# 3. Tuning Grid Specification

# 4. Tuning Using Cross- Validation and the tune_grid() function


# NULL MODEL
Before we can adequately assess if any of our models posess good fit of our data
we need to first create our null model that we can compare the rest of our models
to, if none of our other models perform better than the Null, they are not worth 
pursuing

We need to specify our model before we start computing
*We will use the function "null_model()"*
*Note that we use "classification" for our mode instead of "regression",*
*this is because the mode is predicted for all outcomes, where as continuous,* *number based data would be best with "regression"*
```{r}
null_model1<- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression") %>%
  translate()
```

We will now create our worflow
```{r}
nullmodel_workflow <- workflow() %>% 
  add_model(null_model1) %>%
  add_recipe(recipe_Deaths)
```


We will now compute the performance of a null model for our training and test data
*(doesn't use any predictor information)*


We will now fit our models
We use our deisgnated workflow in fit_resamples followed by our designated name for cross-validation

*Note that our main outcome of interest is cancer deaths, our main predictors of interest are the variables "Race" and "Ethnicity"*
```{r}
train_null_recipe <- fit_resamples(nullmodel_workflow, Cross_Validation_5F, control=control_resamples(verbose=TRUE))

print(train_null_recipe)

collect_metrics(train_null_recipe)
```
This warning would usually be bad on any other model, but since we are receiving it on our null model, we know that it is working as intended

When comparing our models, if the RMSE (our chosen measure of significance) of our other models is worse (in this case a higher value than what is given here, 23678) than what the null model's is, there is indication that the model would not do well at fitting the data. 


# SINGLE TREE MODEL

We will now start with our first comparison model (the single tree model)
Specify our model
```{r}
tune_spec_TREE <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

tune_spec_TREE
```

We will now define the workflow for the tree 
```{r}
workflow_TREE <- workflow() %>%
            add_model(tune_spec_TREE) %>%
            add_recipe(recipe_Deaths) 
```


We will now specify the tuning grid
```{r}
grid_TREE <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
grid_TREE
```
Let's examine this further
```{r}
grid_TREE%>%
    count(tree_depth)
```
Now we will create our workflow
```{r}
res_TREE<-
  workflow_TREE %>%
   tune_grid(resamples = Cross_Validation_5F, grid = grid_TREE, metrics =            metric_set(rmse))
```
Now we will run the autoplot() function to look at some diagnostics
```{r}
res_TREE %>%
  autoplot()
  ggsave(here::here("results", "singletree.png"))
  
  res_TREE%>%show_best()
```

# HOW DOES THIS MODEL PERFORM?
The single tree model with a depth of 15 performed the best with an RMSE of 20315, which is lower than the RMSE of the null model of 23678, this is good since it means that our performance for this model(single tree) was better than the null.


# LASSO
Now we will construct a LASSO model
```{r}
#We will once again start by constructing our model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) 
```
Please note that mixture refers to a number between zero and one that is the 
proportion of L1 regularization (lasso) in the model. In other, words, because
we are using mixture = 1, we are utilizing a "pure" lasso model here

We will now create our workflow
```{r}
lasso_workflow <-workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(recipe_Deaths)
```

Now we will tune our LASSO model
We will utilize parallel computing again to make it faster
```{r}
library(doParallel)
ncores = 5 
cluster <- makePSOCKcluster(5) 
  registerDoParallel(5) 
```

Now we will create our tuning grid 
```{r}
  lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
  #Now we tune the model
  lasso_tune_res <- lasso_workflow %>%
    tune_grid(resamples = Cross_Validation_5F,
              grid = lasso_reg_grid,
              control = control_grid(save_pred = TRUE),
              metrics = metric_set(rmse))
```

We will now turn off parallel clustering, the reason we turn the clustering off
after each use is to prevent computations and analysis from being slowed in 
later data analysis, fitting, modeling, etc.
```{r}
stopCluster(cluster)
```

```{r}
#Now we will view metrics
lasso_tune_res %>% collect_metrics()
```

Now we will select top models
```{r}
top_lasso <- 
  lasso_tune_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso 
```
Now we will see the best lasso out of our five top models
```{r}
best_lasso <- lasso_tune_res %>% select_best()
best_lasso 
```
Our best LASSO model appears to be the one with a penalty of 1

Now we finalize our workflow with the top model and fit it 
```{r}
lasso_topmodel_workflow <- lasso_workflow %>% finalize_workflow(best_lasso)

lasso_topfit<- lasso_topmodel_workflow %>% fit(train_data)

lasso_topfit
```

Now that we have found our top LASSO model, we will now plot it to show quality of fit
```{r}
autoplot(lasso_tune_res)
```
```{r}
#we will now calculate the residuals
lasso_residuals <- lasso_topfit %>%
  augment(train_data) %>% 
  select(.pred, Deaths) %>%
  mutate(.resid = Deaths - .pred) 

lasso_residuals
```

```{r}
#Now we will compare model predictions from tuned model to our actual outcomes
lasso_pred_plot <- ggplot(lasso_residuals, aes(x = Deaths, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Deaths (Outcome)", y = "Deaths (Prediction)")
lasso_pred_plot
```

We will now compare our residuals vs predictions
```{r}
lasso_residuals_plot <- ggplot(lasso_residuals, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "LASSO Model Residuals VS Predictions", x = "Deaths Prediction", y = "Residuals")

lasso_residuals_plot 
```

Finally we will compare our LASSO model to our null model
```{r}
lasso_tune_res %>% show_best(n=1) #view RMSE for best lasso model
```
Our LASSO model has a mean value of 21926.34, which is lower than the null model of 23678, but higher than our single tree model value of 20315. We will finally utilize the RandomForest model to compare quality of fit for our model.


# RANDOMFOREST
So far our single tree model has performed best, we will evaluate the random forest model and compare to determine if we should use the single tree or random forest instead
*Please note that for Random Forest models, "num.threads" and importance*
*is required or else all models will fail*

First we will identify how many cores we can use at one time during parallel processing
```{r}
cores <- parallel::detectCores()
cores
```

We have four cores available for parallel processing, we will use the command "cores" in number of threads to use the max number possible (in our case four)
```{r}
random_forest_model <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")
```

We will now set our workflow
```{r}
random_forest_workflow <- 
  workflow() %>% 
  add_model(random_forest_model) %>% 
  add_recipe(recipe_Deaths)
```

We will now create our tuning grid for our random forest model
```{r}
random_forest_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
```

Let's examine what we are tuning
```{r}
random_forest_model %>% parameters()
```

Now we will tune with our grid that we created
```{r}
random_forest_resampling <- 
  random_forest_workflow %>%
  tune_grid(resamples = Cross_Validation_5F, 
            grid = random_forest_grid, 
            control = control_grid(save_pred = TRUE), 
            metrics = metric_set(rmse))
```

We will now view our top models
```{r}
random_forest_resampling %>% show_best(metric = "rmse")
```

We will now plot our models
```{r}
autoplot(random_forest_resampling)
```

```{r}
#we will now select the best performing model
random_forest_best <- random_forest_resampling %>% select_best(metric = "rmse")

random_forest_best
```


```{r}
#finalize workflow with top model
random_forest_final_workflow <- random_forest_workflow %>% finalize_workflow(random_forest_best)
```


```{r}
#fit model with finalized WF
random_forest_final_fit <- random_forest_final_workflow %>% fit(train_data)

random_forest_final_fit
```

Our random forest model's top model gives an RMSE of 20693, which is slightly worse than our single tree model of 20315, as such, we will use our single tree model to fit out test data
